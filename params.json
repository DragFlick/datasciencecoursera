{"name":"Practical Machine Learning - Final Project Report","tagline":"Machine Learning","body":"\r\n<div id=\"header\">\r\n<h1 class=\"title\">Practical Machine Learning - Final Project Report</h1>\r\n<h4 class=\"author\"><em>Chitresh Pandey</em></h4>\r\n</div>\r\n\r\n\r\n<p>The Project Involves analyzing the Data collected in a controlled experient on 6 subjects. The activity involved here is “Unilateral Dumbbell Biceps Curl” . The data was collected for 5 different methods for the activity denoted as ( A , B , C , D ,E) . Out of 5, only A is the correct method .</p>\r\n<p>A - Exactly as per specification</p>\r\n<p>B - Throwing the elbows to the front</p>\r\n<p>C - Lifting the dumbbell only halfway</p>\r\n<p>D - Lowering the dumbbell only halfway</p>\r\n<p>E - Throwing the hips to the front</p>\r\n<p>Now, here we can pose 2 questions</p>\r\n<p>Q1 . Can we classify that a person is doing the “Unilateral DumbBell Biceps Curl” correctly or incorrectly through a given dataset of specific measurements ?</p>\r\n<p>Q2 . If the person is classified as doing the “Unilateral Dumbbell Biceps Curl” incorrectly , can we further classify him/her into B,C,D or E ?</p>\r\n<p>The data for analysis is provided for by</p>\r\n<p>Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers’ Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.</p>\r\n<p>Loading the necessary libraries and the analysis data . Once the Experiment data is loaded dividing the data into Training and Testing samples .</p>\r\n<p>Loading the Experiment data and slicing it into Testing and Training Sets</p>\r\n<pre class=\"r\"><code>ExpData &lt;- read.csv(&quot;./pml-training.csv&quot;) \r\nset.seed(4868086)\r\ninTrain &lt;- createDataPartition( y = ExpData$classe , p = 0.7 , list = FALSE)\r\nTraining &lt;- ExpData[inTrain , ]\r\nTesting &lt;- ExpData[-inTrain , ]\r\nrm(&quot;inTrain&quot;)</code></pre>\r\n<p>Now creating a Transform function to manage the tranformations that the Training and Testing data may have to undergo.</p>\r\n<pre class=\"r\"><code>Transform &lt;- function(DataSet)\r\n        {\r\n                DataSet[is.na(DataSet)] &lt;- 0\r\n                s_data &lt;- subset(DataSet , \r\n                                 select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp))\r\n                class &lt;- s_data$classe\r\n                s_data &lt;- subset(s_data , select = -classe)\r\n                data &lt;- cbind(class , s_data)\r\n                return(data)\r\n        }</code></pre>\r\n<p>Once we have the Transformed Training and Testing data , identifying the near zero covariates for both Training data and removing them from both Training and Testing dataset.</p>\r\n<pre class=\"r\"><code>Training &lt;- Transform(Training)\r\nTesting  &lt;- Transform(Testing)\r\n\r\n# Identifying the Near Zero CoVariates in Training set and removing them from both Training and Testing data\r\n\r\nNZV &lt;- nearZeroVar(Training , saveMetrics=TRUE)\r\nTraining &lt;- Training[ ,!NZV$nzv]\r\nTesting  &lt;- Testing[  ,!NZV$nzv]</code></pre>\r\n<p>Calculating a CoVariance matrix to identify the Correlated Covariates</p>\r\n<p>Looking at the Co-Relation Matrix we obzerve a high degree of Co-Relation between multiple CoVariates . We go ahead with Principal Componet Analyis (PCA) to reduce the number of Covariates . We go ahead with 2 models within PCA .First Model comprises of 30 PCA out of 54 ( which explains around 98.29% Variablility within the model) and the second model with 40 PCA’s out of 54 ( which explain around 99.8% of the variability)</p>\r\n<p>Model 1 (ModelFit1.PCA30 ) - Applying Random Forest Algorithm to the first 30 Principal Components. Below is the time taken to compute the model</p>\r\n<pre><code>##    user  system elapsed \r\n##   28.01    0.25   28.36</code></pre>\r\n<p>Following are the specifics of the Model with 30 PC</p>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = Training$class ~ ., data = pcompTraining) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 5\r\n## \r\n##         OOB estimate of  error rate: 2.05%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 3880   12    6    5    3 0.006656426\r\n## B   38 2588   25    3    4 0.026335591\r\n## C    6   26 2344   20    0 0.021702838\r\n## D    3    0   95 2147    7 0.046625222\r\n## E    1    5   10   13 2496 0.011485149</code></pre>\r\n<p>Model 2 (ModelFit2.PCA40 ) - Applying Random Forest Algorithm to the first 40 Principal Components. Below is the time taken to compute the model</p>\r\n<pre><code>##    user  system elapsed \r\n##   34.78    0.16   35.05</code></pre>\r\n<p>Following are the specifics of the Model with 40 PC</p>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = Training$class ~ ., data = pcompTraining) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 6\r\n## \r\n##         OOB estimate of  error rate: 1.86%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 3896    5    3    1    1 0.002560164\r\n## B   50 2578   24    2    4 0.030097818\r\n## C    5   22 2347   21    1 0.020450751\r\n## D    3    1   79 2162    7 0.039964476\r\n## E    0    8    8   11 2498 0.010693069</code></pre>\r\n<p>Model 3 (ModelFit3.RF) - Applying Random Forest Algorithm to the data without any PreProcessing. Below is the time taken to compute the model</p>\r\n<pre><code>##    user  system elapsed \r\n##   36.90    0.10   37.03</code></pre>\r\n<p>Following are the specifics of the Model with no data preprocessing</p>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = class ~ ., data = Training) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 7\r\n## \r\n##         OOB estimate of  error rate: 0.27%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 3905    0    0    0    1 0.0002560164\r\n## B    6 2649    3    0    0 0.0033860045\r\n## C    0    6 2390    0    0 0.0025041736\r\n## D    0    0   13 2236    3 0.0071047957\r\n## E    0    0    0    5 2520 0.0019801980</code></pre>\r\n<p>Now Preparing the Principal Componets for the Testing Datasets</p>\r\n<p>Now , Applying the testing datasets to the respective Models to Predict the Outcome</p>\r\n<p>Now Evaluating the confusion matrix for all the 3 model Fit .</p>\r\n<pre class=\"r\"><code>confMat.PCA30 # Confusion Matrix for Model with 30 PCA</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1666    2    3    3    0\r\n##          B   19 1105   14    0    1\r\n##          C    0   12 1008    5    1\r\n##          D    0    1   45  917    1\r\n##          E    0    4    3    5 1070\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9798          \r\n##                  95% CI : (0.9759, 0.9832)\r\n##     No Information Rate : 0.2863          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9744          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9887   0.9831   0.9394   0.9860   0.9972\r\n## Specificity            0.9981   0.9929   0.9963   0.9905   0.9975\r\n## Pos Pred Value         0.9952   0.9701   0.9825   0.9512   0.9889\r\n## Neg Pred Value         0.9955   0.9960   0.9866   0.9974   0.9994\r\n## Prevalence             0.2863   0.1910   0.1823   0.1580   0.1823\r\n## Detection Rate         0.2831   0.1878   0.1713   0.1558   0.1818\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9934   0.9880   0.9678   0.9883   0.9974</code></pre>\r\n<pre class=\"r\"><code>confMat.PCA40 # Confusion Matrix for Model with 40 PCA</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1671    1    1    1    0\r\n##          B   19 1107   13    0    0\r\n##          C    2   10 1008    5    1\r\n##          D    0    0   38  924    2\r\n##          E    0    7    3    3 1069\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.982           \r\n##                  95% CI : (0.9783, 0.9852)\r\n##     No Information Rate : 0.2875          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9772          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9876   0.9840   0.9483   0.9904   0.9972\r\n## Specificity            0.9993   0.9933   0.9963   0.9919   0.9973\r\n## Pos Pred Value         0.9982   0.9719   0.9825   0.9585   0.9880\r\n## Neg Pred Value         0.9950   0.9962   0.9887   0.9982   0.9994\r\n## Prevalence             0.2875   0.1912   0.1806   0.1585   0.1822\r\n## Detection Rate         0.2839   0.1881   0.1713   0.1570   0.1816\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9934   0.9886   0.9723   0.9911   0.9973</code></pre>\r\n<pre class=\"r\"><code>confMat.RF  # Confusion Matrix for Model with no data preprocessing </code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1674    0    0    0    0\r\n##          B    1 1137    1    0    0\r\n##          C    0    5 1021    0    0\r\n##          D    0    0    9  955    0\r\n##          E    0    0    0    3 1079\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9968         \r\n##                  95% CI : (0.995, 0.9981)\r\n##     No Information Rate : 0.2846         \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9959         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9994   0.9956   0.9903   0.9969   1.0000\r\n## Specificity            1.0000   0.9996   0.9990   0.9982   0.9994\r\n## Pos Pred Value         1.0000   0.9982   0.9951   0.9907   0.9972\r\n## Neg Pred Value         0.9998   0.9989   0.9979   0.9994   1.0000\r\n## Prevalence             0.2846   0.1941   0.1752   0.1628   0.1833\r\n## Detection Rate         0.2845   0.1932   0.1735   0.1623   0.1833\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9997   0.9976   0.9946   0.9975   0.9997</code></pre>\r\n<p>Consolidating the data for all the 3 models in a single data frame for comparisons</p>\r\n<pre><code>##                  Accuracy ElapsedTime\r\n## ModelFit1.PCA30 0.9797791       28.36\r\n## ModelFit1.PCA40 0.9819881       35.05\r\n## ModelFit1.RF    0.9967715       37.03</code></pre>\r\n<p>We see that the PCA model with 30 Principal Componets suffers from an accuracy trade off as compared to Model without any PreProcessing . However , if we look at the execution time we observer that there is a significant execution time gain in PCA30 Model (ModelFit1.PCA30) with respect to the Model without any PreProcessing (ModelFit3.RF).</p>\r\n<p>For the case in discussion here, we will consider the solution that has the maximum Overall Accuracy. We will go ahead with the model with no data preprocessing (ModelFit3.RF).</p>\r\n<p>Now , Importing the Validation data and predicting the results through model ModelFit3.RF</p>\r\n<pre class=\"r\"><code>Validation &lt;- read.csv(&quot;./pml-testing.csv&quot;)\r\nValResult &lt;- predict(ModelFit3.RF , Validation)</code></pre>\r\n<p>Submitted the Validation result for final evaluation .</p>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}