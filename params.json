{"name":"Practical Machine Learning - Final Project Report","tagline":"Machine Learning","body":"<div id=\"header\">\r\n<h1 class=\"title\">Practical Machine Learning - Final Project Report</h1>\r\n<h4 class=\"author\"><em>Chitresh Pandey</em></h4>\r\n</div>\r\n\r\n\r\n<p>The Project Involves analyzing the Data collected in a controlled experient on 6 subjects. The activity involved here is “Unilateral Dumbbell Biceps Curl” . The data was collected for 5 different methods for the activity denoted as ( A , B , C , D ,E) . Out of 5, only A is the correct method .</p>\r\n<p>A - Exactly as per specification</p>\r\n<p>B - Throwing the elbows to the front</p>\r\n<p>C - Lifting the dumbbell only halfway</p>\r\n<p>D - Lowering the dumbbell only halfway</p>\r\n<p>E - Throwing the hips to the front</p>\r\n<p>Now, here we can pose 2 questions</p>\r\n<p>Q1 . Can we classify that a person is doing the “Unilateral Dumbbell Biceps Curl” correctly or incorrectly through a given dataset of specific measurements ?</p>\r\n<p>Q2 . If the person is classified as doing the “Unilateral Dumbbell Biceps Curl” incorrectly , can we further classify him/her into B,C,D or E ?</p>\r\n<p>The data for analysis is provided for by</p>\r\n<p>Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers’ Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.</p>\r\n<p>Loading the necessary libraries and the analysis data . Once the Experiment data is loaded dividing the data into Training and Testing samples .</p>\r\n<p>Loading the Experiment data and slicing it into Testing and Training Sets</p>\r\n<pre class=\"r\"><code>ExpData &lt;- read.csv(&quot;./pml-training.csv&quot;) \r\nset.seed(4868086)\r\ninTrain &lt;- createDataPartition( y = ExpData$classe , p = 0.7 , list = FALSE)\r\nTraining &lt;- ExpData[inTrain , ]\r\nTesting &lt;- ExpData[-inTrain , ]\r\nrm(&quot;inTrain&quot;)</code></pre>\r\n<p>Now creating a Transform function to manage the tranformations that the Training and Testing data may have to undergo.</p>\r\n<p>Once we have the Transformed Training and Testing data , identifying the near zero covariates for both Training data and removing them from both Training and Testing dataset.</p>\r\n<pre class=\"r\"><code>Training &lt;- Transform(Training)\r\nTesting  &lt;- Transform(Testing)\r\n\r\n# Identifying the Near Zero CoVariates in Training set and removing them from both Training and Testing data\r\n\r\nNZV &lt;- nearZeroVar(Training , saveMetrics=TRUE)\r\nTraining &lt;- Training[ ,!NZV$nzv]\r\nTesting  &lt;- Testing[  ,!NZV$nzv]</code></pre>\r\n<p>Now , preparing my model on the training data .</p>\r\n<pre class=\"r\"><code>ModelFit1 &lt;- update(ModelFit , data = Training) # Random Forest </code></pre>\r\n<pre class=\"r\"><code>ModelFit2 &lt;- update(ModelFit ,method  = &quot;gbm&quot;,data = Training,verbose = FALSE) # Boosting</code></pre>\r\n<p>Following are the statistics for both the Model Fits .</p>\r\n<pre class=\"r\"><code>print(ModelFit1) # Random Forest </code></pre>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = class ~ ., data = Training) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 7\r\n## \r\n##         OOB estimate of  error rate: 0.25%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 3905    0    0    0    1 0.0002560164\r\n## B    6 2649    3    0    0 0.0033860045\r\n## C    0    5 2391    0    0 0.0020868114\r\n## D    0    0   14 2235    3 0.0075488455\r\n## E    0    0    0    2 2523 0.0007920792</code></pre>\r\n<pre class=\"r\"><code>print(ModelFit2) # Boosting </code></pre>\r\n<pre><code>## Stochastic Gradient Boosting \r\n## \r\n## 1000 samples\r\n##   53 predictor\r\n##    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n## \r\n## No pre-processing\r\n## Resampling: Bootstrapped (25 reps) \r\n## \r\n## Summary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... \r\n## \r\n## Resampling results across tuning parameters:\r\n## \r\n##   interaction.depth  n.trees  Accuracy   Kappa      Accuracy SD\r\n##   1                   50      0.7051253  0.6259041  0.02964519 \r\n##   1                  100      0.7611809  0.6975828  0.02374762 \r\n##   1                  150      0.7905237  0.7348195  0.01845057 \r\n##   2                   50      0.7923912  0.7370063  0.02181351 \r\n##   2                  100      0.8408204  0.7985020  0.01457378 \r\n##   2                  150      0.8599204  0.8226528  0.01647126 \r\n##   3                   50      0.8315520  0.7867569  0.01829320 \r\n##   3                  100      0.8656772  0.8299641  0.01570775 \r\n##   3                  150      0.8756687  0.8426158  0.01471798 \r\n##   Kappa SD  \r\n##   0.03757160\r\n##   0.03014136\r\n##   0.02338426\r\n##   0.02753128\r\n##   0.01828226\r\n##   0.02052475\r\n##   0.02303353\r\n##   0.01984513\r\n##   0.01850295\r\n## \r\n## Tuning parameter 'shrinkage' was held constant at a value of 0.1\r\n## Accuracy was used to select the optimal model using  the largest value.\r\n## The final values used for the model were n.trees = 150,\r\n##  interaction.depth = 3 and shrinkage = 0.1.</code></pre>\r\n<p>Now predicting the output for the Testing data sets from the Models</p>\r\n<pre class=\"r\"><code>ResultSet1 &lt;- predict(ModelFit1 , Testing)\r\nResultSet2 &lt;- predict(ModelFit2 , Testing)</code></pre>\r\n<p>Evaluating the Confusion Matrix for the two Models to compare the estimated Out of Sample error rates</p>\r\n<pre class=\"r\"><code>confusionMatrix(Testing$class , ResultSet1)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1674    0    0    0    0\r\n##          B    1 1136    2    0    0\r\n##          C    0    3 1023    0    0\r\n##          D    0    0    9  955    0\r\n##          E    0    0    0    3 1079\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9969          \r\n##                  95% CI : (0.9952, 0.9982)\r\n##     No Information Rate : 0.2846          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9961          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9994   0.9974   0.9894   0.9969   1.0000\r\n## Specificity            1.0000   0.9994   0.9994   0.9982   0.9994\r\n## Pos Pred Value         1.0000   0.9974   0.9971   0.9907   0.9972\r\n## Neg Pred Value         0.9998   0.9994   0.9977   0.9994   1.0000\r\n## Prevalence             0.2846   0.1935   0.1757   0.1628   0.1833\r\n## Detection Rate         0.2845   0.1930   0.1738   0.1623   0.1833\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9997   0.9984   0.9944   0.9975   0.9997</code></pre>\r\n<pre class=\"r\"><code>confusionMatrix(Testing$class , ResultSet2)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1651   12    4    5    2\r\n##          B   81 1014   37    4    3\r\n##          C    1   52  954   19    0\r\n##          D    3   14   57  871   19\r\n##          E    2   42    9   42  987\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9307         \r\n##                  95% CI : (0.9239, 0.937)\r\n##     No Information Rate : 0.2953         \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9122         \r\n##  Mcnemar's Test P-Value : &lt; 2.2e-16      \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9499   0.8942   0.8992   0.9256   0.9763\r\n## Specificity            0.9945   0.9737   0.9851   0.9812   0.9805\r\n## Pos Pred Value         0.9863   0.8903   0.9298   0.9035   0.9122\r\n## Neg Pred Value         0.9793   0.9747   0.9780   0.9858   0.9950\r\n## Prevalence             0.2953   0.1927   0.1803   0.1599   0.1718\r\n## Detection Rate         0.2805   0.1723   0.1621   0.1480   0.1677\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9722   0.9339   0.9421   0.9534   0.9784</code></pre>\r\n<p>On comapring the confusion Matrix for both the Model Fit , we observe that the Random Forest provides a better prediction estimates and lower out of sample error rates as compared to Boosting . Hence , implementing the Random Forest Model to our validation dataset ( ModelFit1 )</p>\r\n<p>Now , as we have finalized our Model , importing the data for Validation</p>\r\n<pre class=\"r\"><code>Validation &lt;- read.csv(&quot;./pml-testing.csv&quot;)\r\nValResult &lt;- predict(ModelFit1 , Validation)</code></pre>\r\n<p>Submitted the results of Validation data set .</p>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}